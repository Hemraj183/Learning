{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "alert": "info"
   },
   "source": [
    "# \ud83d\ude80 Interactive Mode Available!\n",
    "\n",
    "Typical static notebooks are boring. We have a dedicated interactive module for this week.\n",
    "\n",
    "[\ud83d\udc49 **Click here to open the Interactive Visualization**](../../interactive_platform/modules/week3_llm_variants/interactive.html)\n",
    "\n",
    "*(Note: Open this link in a new tab to keep the notebook running)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "alert": "info"
   },
   "source": [
    "# \ud83d\ude80 Interactive Mode Available!\n",
    "\n",
    "Typical static notebooks are boring. We have a dedicated interactive module for this week.\n",
    "\n",
    "[\ud83d\udc49 **Click here to open the Interactive Visualization**](../../interactive_platform/modules/week3_llm_variants/interactive.html)\n",
    "\n",
    "*(Note: Open this link in a new tab to keep the notebook running)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: LLM Variants & Latent Spaces\n",
    "\n",
    "Welcome to Week 3! We are moving from structure (Transformers) to actual models (GPT-2) and concepts (Latent Spaces).\n",
    "\n",
    "## Goals:\n",
    "1.  **Tokenization**: Understand how text becomes numbers.\n",
    "2.  **GPT-2**: Verify the \"Decoder-Only\" architecture.\n",
    "3.  **Latent Space**: Visualize word relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "print(\"GPT-2 Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization Deep Dive\n",
    "\n",
    "GPT-2 uses Byte-Pair Encoding (BPE). It splits common words into subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n",
    "\n",
    "# Try a complex word\n",
    "complex_word = \"antigravity\"\n",
    "print(f\"\\n'{complex_word}' tokens:\", tokenizer.tokenize(complex_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Generation\n",
    "\n",
    "Let's see the model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=20):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate\n",
    "    out = model.generate(\n",
    "        input_ids, \n",
    "        max_length=max_length, \n",
    "        do_sample=True,    # Random sampling\n",
    "        temperature=0.7    # Creativity control\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate(\"Once upon a time in AI,\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Investigating Latent Spaces\n",
    "\n",
    "We can look at the embedding layer `wte` (Word Token Embeddings) to see how the model groups words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [(\"King\", \"Queen\"), (\"Man\", \"Woman\"), (\"Paris\", \"France\"), (\"Apple\", \"Car\")]\n",
    "\n",
    "embeddings = model.transformer.wte.weight  # [vocab_size, n_embd]\n",
    "\n",
    "def get_sim(w1, w2):\n",
    "    id1 = tokenizer.encode(w1)[0]\n",
    "    id2 = tokenizer.encode(w2)[0]\n",
    "    e1 = embeddings[id1]\n",
    "    e2 = embeddings[id2]\n",
    "    \n",
    "    # Cosine similarity\n",
    "    sim = torch.nn.functional.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0))\n",
    "    return sim.item()\n",
    "\n",
    "for w1, w2 in word_pairs:\n",
    "    print(f\"Sim({w1}, {w2}) = {get_sim(w1, w2):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}