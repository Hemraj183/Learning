import torch 

def exercise_load_balancing():
    # TODO: MoE suffers from "Expert Collapse" (one expert does everything).
    # Implement an auxiliary noise loss or load balancing loss to ensure experts are used equally.
    print("Exercise: Research Load Balancing Loss.")

if __name__ == "__main__":
    exercise_load_balancing()
