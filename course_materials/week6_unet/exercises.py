import torch
import torch.nn as nn

def exercise_attention_gate():
    # TODO: Research and implement an "Attention Gate" for the skip connections
    # This allows the decoder to focus only on relevant parts of the encoder features
    print("Exercise: Add Attention Gates to U-Net.")

if __name__ == "__main__":
    exercise_attention_gate()
