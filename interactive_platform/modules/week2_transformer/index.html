<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 2: The Transformer Architecture | Deep Learning Mastery</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Global Progress Tracker (Hidden Metadata) -->
    <div id="course-metadata" data-current-week="week2_transformer" style="display:none;"></div>

    <!-- Sidebar -->
    <nav class="sidebar">
        <div class="sidebar-header">
            <h2>Transformer Arch</h2>
            <div class="progress-container">
                 <div class="progress-bar"><div class="progress-fill" id="page-progress"></div></div>
                 <span class="progress-text" id="progress-text">0% Complete</span>
            </div>
            <a href="../../index.html" class="back-link">‚Üê Course Dashboard</a>
        </div>
        
        <div class="nav-scroll-area">
            <h3 class="nav-section-title">This Module</h3>
            <ul class="nav-links local-nav">
                <li><a href="#intro">üìñ Introduction</a></li>
                <li><a href="#theory">üß† Theory & Math</a></li>
                <li><a href="#interactive">üî¨ Interactive Lab</a></li>
                <li><a href="#code">üíª Implementation</a></li>
                <li><a href="#project">üéØ Project</a></li>
            </ul>

            <h3 class="nav-section-title" style="margin-top:20px;">Course Map</h3>
            <ul class="nav-links global-nav">
                <li><a href="../week1_pytorch/index.html" class="week-link" data-week="week1_pytorch">Week 1: PyTorch</a></li>
                <li><a href="../week2_transformer/index.html" class="week-link" data-week="week2_transformer">Week 2: Transformers</a></li>
                <li><a href="../week3_llm_variants/index.html" class="week-link" data-week="week3_llm_variants">Week 3: LLMs</a></li>
                <li><a href="../week4_router/index.html" class="week-link" data-week="week4_router">Week 4: Router</a></li>
                <li><a href="../week5_diffusion/index.html" class="week-link" data-week="week5_diffusion">Week 5: Diffusion</a></li>
                <li><a href="../week6_unet/index.html" class="week-link" data-week="week6_unet">Week 6: U-Net</a></li>
                <li><a href="../week7_ldm/index.html" class="week-link" data-week="week7_ldm">Week 7: Latent Diff.</a></li>
                <li><a href="../week8_capstone/index.html" class="week-link" data-week="week8_capstone">Week 8: Capstone</a></li>
                <li><a href="../week9_lora/index.html" class="week-link" data-week="week9_lora">Week 9: LoRA</a></li>
                <li><a href="../week10_moe/index.html" class="week-link" data-week="week10_moe">Week 10: MoE</a></li>
                <li><a href="../week11_opt/index.html" class="week-link" data-week="week11_opt">Week 11: Opt</a></li>
                <li><a href="../week12_capstone/index.html" class="week-link" data-week="week12_capstone">Week 12: Final</a></li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="content">
        <!-- Hero -->
        <section id="intro" class="hero-section">
            <h1 class="gradient-text">Week 2: The Transformer Architecture</h1>
            <p class="subtitle">Attention Is All You Need: The architecture that changed modern AI forever.</p>
            <div class="content-card">
                <h3>Overview</h3>
                
            <p>Before Transformers, NLP relied on RNNs and LSTMs, which processed data sequentially (slow) and struggled with long-range dependencies. The <strong>Transformer</strong> (2017) revolutionized this by introducing <strong>Self-Attention</strong> and <strong>Parallelization</strong>.</p>
            <p>In this week, we dissect the architecture block by block: Positional Encoding, Multi-Head Attention, and Feed-Forward Networks.</p>
        
                <div class="checkpoint">
                    <input type="checkbox" id="check-intro" class="checkpoint-input" onchange="updateProgress()">
                    <label for="check-intro">I understand the goals of this week</label>
                </div>
            </div>
        </section>

        <!-- Theory -->
        <section id="theory" class="section">
            <h2 class="section-title">Theory & Concepts</h2>
            
            <div class="content-card">
                <h3>The Core Mechanism: Self-Attention</h3>
                <p>Self-attention allows the model to look at other words in the sentence to build a better representation of the current word. For example, in "The animal didn't cross the street because <strong>it</strong> was too tired", attention links "it" to "animal".</p>
                <h4>The Equation</h4>
                <div class="code-block">Attention(Q, K, V) = softmax( (QK^T) / ‚àöd_k ) V</div>
                <ul>
                    <li><strong>Query (Q):</strong> What we are looking for.</li>
                    <li><strong>Key (K):</strong> What the token offers.</li>
                    <li><strong>Value (V):</strong> The actual information content.</li>
                </ul>
            </div>
            <div class="content-card">
                <h3>Positional Encoding</h3>
                <p>Since the Transformer has no recurrence, it has no inherent notion of order. We must inject position information.</p>
                <div class="code-block">PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</div>
            </div>
        
             <div class="content-card">
                 <h3>Concept Check</h3>
                 <p>Make sure you grasp the core equations before moving to code.</p>
                 <div class="checkpoint">
                    <input type="checkbox" id="check-theory" class="checkpoint-input" onchange="updateProgress()">
                    <label for="check-theory">I have reviewed the theoretical foundations</label>
                </div>
             </div>
        </section>

        <!-- Interactive -->
        <section id="interactive" class="section">
            <h2 class="section-title">Interactive Lab: Multi-Head Attention</h2>
            <div class="content-card viz-card">
                <p>Visualize how different heads focus on different parts of the sentence. Click 'Simulate' to see new patterns.</p>
                <div class="viz-container" id="viz-container">
                    <svg id="attn-viz" width="600" height="300"></svg><br><button onclick="visualizeAttention()">Simulate Attention Weights</button>
                </div>
                <div class="info-box"><strong>üí° Experiment:</strong> Try different values to see how the system reacts in real-time.</div>
            </div>
        </section>

        <!-- Implementation -->
        <section id="code" class="section">
            <h2 class="section-title">Code Implementation</h2>
            <div class="content-card">
                <h3>Core Logic</h3>
                <p>Below is the critical implementation from <code>project.py</code>. Analyze how the theory transforms into PyTorch code.</p>
                <div class="code-block">
                    <div class="code-header">Python <button class="copy-btn" onclick="copyCode()">Copy</button></div>
                    <pre><code id="main-code">class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch, seq_len, _ = x.shape
        
        # 1. Linear Projections & Split Heads
        Q = self.W_q(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # 2. Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attn = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn, V)
        
        # 3. Concatenate & Final Linear
        output = output.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)
        return self.W_o(output)</code></pre>
                </div>
                 <div class="checkpoint">
                    <input type="checkbox" id="check-code" class="checkpoint-input" onchange="updateProgress()">
                    <label for="check-code">I understand the code implementation</label>
                </div>
            </div>
        </section>
        
        <!-- Project -->
        <section id="project" class="section">
             <h2 class="section-title">Weekly Project</h2>
             <div class="content-card">
                <h3>Build a Transformer Block</h3>
                <p>You will implement a reusable Transformer Encoder Layer containing Multi-Head Attention and a FeedForward Network.</p>
                <div class="project-specs">
                    <h4>Step-by-Step Implementation</h4>
                    <ul class="checklist">
                        
            <li>Implement <code>PositionalEncoding</code> class using sine/cosine functions.</li>
            <li>Implement <code>MultiHeadAttention</code> with correct tensor reshaping.</li>
            <li>Build the <code>FeedForward</code> block (Linear -> ReLU -> Linear).</li>
            <li>Assemble them into a <code>TransformerBlock</code> with LayerNorm and Residual Connections.</li>
        
                    </ul>
                </div>
                <div class="info-box">
                    <strong>üöÄ Challenge:</strong> Can you optimize this further? Check the <code>exercises.py</code> file for bonus tasks.
                </div>
                 <div class="checkpoint">
                    <input type="checkbox" id="check-project" class="checkpoint-input" onchange="updateProgress()">
                    <label for="check-project">I have completed the weekly project</label>
                </div>
             </div>
        </section>

        <footer>
            <div class="nav-buttons">
                <a href="../../modules/week1_pytorch/index.html" class="btn-secondary">‚Üê Previous Week</a>
                <a href="../../modules/week3_llm_variants/index.html" class="btn-primary">Next Week ‚Üí</a>
            </div>
            <p style="margin-top: 40px; color: var(--text-muted);">Antigravity Learning System &copy; 2026</p>
        </footer>
    </main>

    <script src="script.js"></script>
    <script>
        // Global Progress Logic
        const CURRENT_WEEK = "week2_transformer";
        
        function updateProgress() {
            const checks = document.querySelectorAll('.checkpoint-input');
            const checked = document.querySelectorAll('.checkpoint-input:checked');
            const progress = (checked.length / checks.length) * 100;
            
            document.getElementById('page-progress').style.width = `${progress}%`;
            document.getElementById('progress-text').innerText = `${Math.round(progress)}% Complete`;
            
            // Save to localStorage
            const state = {};
            checks.forEach(c => state[c.id] = c.checked);
            localStorage.setItem(`progress_${CURRENT_WEEK}`, JSON.stringify(state));
            
            // Mark week as done if > 90%
            if (progress > 90) {
                localStorage.setItem(`status_${CURRENT_WEEK}`, 'done');
            }
        }

        function loadProgress() {
             const saved = JSON.parse(localStorage.getItem(`progress_${CURRENT_WEEK}`));
             if (saved) {
                 Object.keys(saved).forEach(id => {
                     const el = document.getElementById(id);
                     if(el) el.checked = saved[id];
                 });
                 updateProgress();
             }
             
             // Update sidebar ticks
             document.querySelectorAll('.week-link').forEach(link => {
                 const week = link.getAttribute('data-week');
                 if (localStorage.getItem(`status_${week}`) === 'done') {
                     link.classList.add('done-week');
                 }
                 if (week === CURRENT_WEEK) link.classList.add('active-week');
             });
        }
        
        function copyCode() {
            const code = document.getElementById('main-code').innerText;
            navigator.clipboard.writeText(code);
            alert('Code copied!');
        }

        window.onload = loadProgress;
    </script>
</body>
</html>
