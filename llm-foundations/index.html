<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Engine: Attention is All You Need</title>
    <meta name="description" content="Interactive deep dive into the Transformer Architecture and Self-Attention">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <!-- Animated Background -->
    <div class="animated-bg"></div>

    <!-- Navigation Sidebar -->
    <nav class="sidebar">
        <div class="sidebar-header">
            <h1>Transformer<br><span class="gradient-text">Engine</span></h1>
            <div class="progress-ring">
                <svg width="60" height="60">
                    <circle class="progress-ring-circle" stroke-width="4" fill="transparent" r="26" cx="30" cy="30" />
                </svg>
                <span class="progress-text">0%</span>
            </div>
        </div>
        <ul class="nav-links">
            <li><a href="../index.html" class="hub-link">üè† Main Menu</a></li>
            <li><a href="../pytorch-tutorial/index.html" class="switch-link">üß† Switch to Week 1</a></li>
            <li class="nav-divider"></li>
            <li><a href="#intro" class="active">Introduction</a></li>
            <li><a href="#self-attention">Self-Attention</a></li>
            <li><a href="#multi-head">Multi-Head Attention</a></li>
            <li><a href="#qkv-logic">QKV Mechanism</a></li>
            <li><a href="#positional">Positional Encoding</a></li>
            <li><a href="#block">Transformer Block</a></li>
            <li><a href="#exercises">Exercises</a></li>
        </ul>
    </nav>

    <!-- Main Content -->
    <main class="content">
        <!-- Hero Section -->
        <section class="hero">
            <h1 class="hero-title">The Transformer Architecture</h1>
            <p class="hero-subtitle">Visualizing the engine of modern Large Language Models</p>
            <div class="hero-features">
                <div class="feature-card">
                    <div class="feature-icon">üîç</div>
                    <h3>Attention</h3>
                    <p>Weighting importance</p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">üß©</div>
                    <h3>QKV</h3>
                    <p>The routing logic</p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">üìê</div>
                    <h3>PE</h3>
                    <p>Order without recurrence</p>
                </div>
            </div>
        </section>

        <!-- Introduction Section -->
        <section id="intro" class="section">
            <h2 class="section-title">Why Transformers?</h2>
            <div class="content-card">
                <h3>Beyond RNNs and LSTMs</h3>
                <p>Before Transformers, sequential data was processed one-by-one (Recurrence). This was slow and had a
                    "vocabulary" memory problem. Transformers changed everything by allowing for **Parallel
                    Processing**.</p>
                <ul>
                    <li><strong>No Recurrence:</strong> Process 1000 tokens at once.</li>
                    <li><strong>Global Context:</strong> Every token can "see" every other token directly.</li>
                    <li><strong>Scaling:</strong> The foundation for GPT-4, Llama, and Claude.</li>
                </ul>

                <div class="info-box">
                    <strong>üí° Key Insight:</strong> Transformers replaced "Time" (recurrence) with "Attention"
                    (weighting).
                </div>

                <div class="checkpoint">
                    <input type="checkbox" id="check-intro" class="checkpoint-input">
                    <label for="check-intro">I understand the fundamental shift from RNNs to Transformers</label>
                </div>
            </div>
        </section>

        <!-- Self-Attention Section -->
        <section id="self-attention" class="section">
            <h2 class="section-title">Self-Attention</h2>
            <div class="content-card">
                <h3>What is "Attention"?</h3>
                <p>Attention allows the model to focus on the most relevant parts of the input. When processing the word
                    **"bank"**, attention decides if the model should look at **"river"** or **"money"** to understand
                    the context.</p>

                <div class="visualization-box">
                    <h4>Interactive Attention Matrix</h4>
                    <p>Click on a word to see how it "attends" to others.</p>
                    <div id="attention-viz" class="viz-container">
                        <!-- Attention Matrix will be injected here -->
                        <div class="attention-tokens">
                            <span class="token" data-index="0">The</span>
                            <span class="token" data-index="1">animal</span>
                            <span class="token" data-index="2">didn't</span>
                            <span class="token" data-index="3">cross</span>
                            <span class="token" data-index="4">the</span>
                            <span class="token" data-index="5">street</span>
                            <span class="token" data-index="6">because</span>
                            <span class="token" data-index="7">it</span>
                            <span class="token" data-index="8">was</span>
                            <span class="token" data-index="9">too</span>
                            <span class="token" data-index="10">tired</span>
                        </div>
                        <div id="attention-matrix" class="matrix-grid"></div>
                    </div>
                </div>

                <div class="checkpoint">
                    <input type="checkbox" id="check-attention" class="checkpoint-input">
                    <label for="check-attention">I understand the concept of weighted relevance</label>
                </div>
            </div>
        </section>

        <!-- Multi-Head Attention Section -->
        <section id="multi-head" class="section">
            <h2 class="section-title">Multi-Head Attention</h2>
            <div class="content-card">
                <h3>Why Multiple Heads?</h3>
                <p>One attention head might focus on **Grammar**, while another focuses on **Entities**, and a third on
                    **Sentiment**. By using multiple "heads", the model can capture different types of relationships
                    simultaneously.</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-lang">Python</span>
                        <button class="copy-btn" data-copy="code-mha">Copy</button>
                    </div>
                    <pre><code id="code-mha"># Simplified Multi-Head Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(MultiHeadAttention, self).__init__()
        self.heads = heads
        self.head_dim = embed_size // heads
        
        # Q, K, V Projections
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)</code></pre>
                </div>

                <div class="checkpoint">
                    <input type="checkbox" id="check-mha" class="checkpoint-input">
                    <label for="check-mha">I understand why we split attention into multiple heads</label>
                </div>
            </div>
        </section>

        <!-- QKV Mechanism Section -->
        <section id="qkv-logic" class="section">
            <h2 class="section-title">The QK<sup>T</sup>V Mechanism</h2>
            <div class="content-card">
                <h3>Query, Key, and Value</h3>
                <p>Imagine a Library search:</p>
                <ul>
                    <li><strong>Query (Q):</strong> What you are looking for (e.g., "Deep Learning books").</li>
                    <li><strong>Key (K):</strong> The labels on the books in the library.</li>
                    <li><strong>Value (V):</strong> The actual content of the books that match.</li>
                </ul>

                <div class="math-box">
                    \[ Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \]
                </div>

                <div class="visualization-box">
                    <h4>QKV Routing Simulator</h4>
                    <div id="qkv-viz" class="viz-container">
                        <!-- QKV Animation will be here -->
                    </div>
                </div>

                <div class="checkpoint">
                    <input type="checkbox" id="check-qkv" class="checkpoint-input">
                    <label for="check-qkv">I understand the analogy of Q, K, and V</label>
                </div>
            </div>
        </section>

        <!-- Positional Encoding Section -->
        <section id="positional" class="section">
            <h2 class="section-title">Positional Encoding</h2>
            <div class="content-card">
                <h3>Solving the Order Problem</h3>
                <p>Since Transformers process tokens in parallel, they don't know if "The cat sat on the mat" or "The
                    mat sat on the cat". We add **Positional Encodings** to the embeddings to give them a sense of
                    "where" they are.</p>

                <div class="visualization-box">
                    <h4>Sinusoidal Wave Visualization</h4>
                    <canvas id="pe-canvas" width="600" height="200"></canvas>
                </div>

                <div class="checkpoint">
                    <input type="checkbox" id="check-pe" class="checkpoint-input">
                    <label for="check-pe">I understand how order is injected into the model</label>
                </div>
            </div>
        </section>

        <!-- Transformer Block Section -->
        <section id="block" class="section">
            <h2 class="section-title">The Transformer Block</h2>
            <div class="content-card">
                <h3>Putting it All Together</h3>
                <p>A standard Transformer Block consists of Two main sub-layers with **Residual Connections** and
                    **Layer Normalization**.</p>
                <ol>
                    <li>Multi-Head Self-Attention</li>
                    <li>Feed-Forward Neural Network</li>
                </ol>

                <div class="success-box">
                    <strong>üöÄ Final Step:</strong> Implement the block in the lab below!
                </div>

                <div class="checkpoint">
                    <input type="checkbox" id="check-block" class="checkpoint-input">
                    <label for="check-block">I'm ready to implement the Transformer Block</label>
                </div>
            </div>
        </section>

        <!-- Exercises Section -->
        <section id="exercises" class="section">
            <h2 class="section-title">Week 2 Lab</h2>
            <div class="content-card">
                <h3>Coding Challenge: TransformerBlock from Scratch</h3>
                <p>Complete the implementation of the <code>TransformerBlock</code>. Use the terminal below to test your
                    code.</p>

                <div class="exercise-card">
                    <h4>Goal: Implement the Forward Pass</h4>
                    <p>Ensure you apply LayerNorm and Residual Connections correctly.</p>
                    <details>
                        <summary>View Solution Sketch</summary>
                        <div class="code-block">
                            <pre><code>def forward(self, value, key, query, mask):
    attention = self.attention(value, key, query, mask)
    x = self.dropout(self.norm1(attention + query)) # Residual 1
    forward = self.feed_forward(x)
    out = self.dropout(self.norm2(forward + x))      # Residual 2
    return out</code></pre>
                        </div>
                    </details>
                </div>
            </div>
        </section>

        <!-- Footer -->
        <footer class="footer">
            <p>Built with ‚ù§Ô∏è for LLM learners | Week 2 Completion üöÄ</p>
        </footer>
    </main>

    <!-- Python Terminal -->
    <div class="python-terminal" id="python-terminal">
        <div class="terminal-header">
            <div class="terminal-title">
                <span class="terminal-icon">üêç</span>
                <span>Transformer Lab</span>
                <span class="terminal-status" id="terminal-status">Loading...</span>
            </div>
            <div class="terminal-controls">
                <button class="terminal-btn" id="minimize-terminal">‚àí</button>
                <button class="terminal-btn" id="close-terminal">√ó</button>
            </div>
        </div>
        <div class="terminal-body">
            <div class="terminal-editor">
                <textarea id="python-code" class="code-input"
                    placeholder="# Implement TransformerBlock here..."></textarea>
            </div>
            <div class="terminal-actions">
                <button class="run-btn" id="run-code"><span class="btn-icon">‚ñ∂</span> Run Code</button>
                <button class="clear-btn" id="clear-output">Clear</button>
                <select id="example-snippets" class="snippet-select">
                    <option value="">Load Snippet...</option>
                    <option value="attention">Self-Attention</option>
                    <option value="transformer">Transformer Block</option>
                </select>
            </div>
            <div class="terminal-output" id="terminal-output">
                <div class="output-placeholder">Run code to see results...</div>
            </div>
        </div>
    </div>

    <!-- Terminal Toggle -->
    <button class="terminal-toggle" id="terminal-toggle">
        <span>üêç</span>
        <span>Open Lab</span>
    </button>

    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="script.js"></script>
</body>

</html>